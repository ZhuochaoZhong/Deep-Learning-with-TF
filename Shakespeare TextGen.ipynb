{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('alllines.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = text[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ACT I\"\n",
      "\"SCENE I. London. The palace.\"\n",
      "\"Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others\"\n",
      "\"So shaken as we are, so wan with care,\"\n",
      "\"Find we a time for frighted peace to pant,\"\n",
      "\"And breathe short-winded accents of new broils\"\n",
      "\"To be commenced in strands afar remote.\"\n",
      "\"No more the thirsty entrance of this soil\"\n",
      "\"Shall daub her lips with her own children's blood,\"\n",
      "\"Nor more shall trenching war channel her fields,\"\n",
      "\"Nor bruise her flowerets with the armed hoofs\"\n"
     ]
    }
   ],
   "source": [
    "print(sample_text[:510])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(sample_text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {c : i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char_to_idx[c] for c in sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\"ACT I\"\\n\"SCENE I. London. The palace.\"\\n\"Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMOR'\n",
      "'ELAND, SIR WALTER BLUNT, and others\"\\n\"So shaken as we are, so wan with care,\"\\n\"Find we a time for fri'\n",
      "'ghted peace to pant,\"\\n\"And breathe short-winded accents of new broils\"\\n\"To be commenced in strands af'\n",
      "'ar remote.\"\\n\"No more the thirsty entrance of this soil\"\\n\"Shall daub her lips with her own children\\'s '\n",
      "'blood,\"\\n\"Nor more shall trenching war channel her fields,\"\\n\"Nor bruise her flowerets with the armed h'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 4, 16, 18, 35,  2, 24,  4,  1,  4, 34, 18, 20, 29, 20,  2, 24,  8,\n",
      "        2, 27, 58, 57, 47, 58, 57,  8,  2, 35, 51, 48,  2, 59, 44, 55, 44,\n",
      "       46, 48,  8,  4,  1,  4, 20, 57, 63, 48, 61,  2, 26, 24, 29, 22,  2,\n",
      "       23, 20, 29, 33, 40,  6,  2, 27, 30, 33, 19,  2, 25, 30, 23, 29,  2,\n",
      "       30, 21,  2, 27, 16, 29, 18, 16, 34, 35, 20, 33,  6,  2, 63, 51, 48,\n",
      "        2, 20, 16, 33, 27,  2, 58, 49,  2, 38, 20, 34, 35, 28, 30])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([16, 18, 35,  2, 24,  4,  1,  4, 34, 18, 20, 29, 20,  2, 24,  8,  2,\n",
      "       27, 58, 57, 47, 58, 57,  8,  2, 35, 51, 48,  2, 59, 44, 55, 44, 46,\n",
      "       48,  8,  4,  1,  4, 20, 57, 63, 48, 61,  2, 26, 24, 29, 22,  2, 23,\n",
      "       20, 29, 33, 40,  6,  2, 27, 30, 33, 19,  2, 25, 30, 23, 29,  2, 30,\n",
      "       21,  2, 27, 16, 29, 18, 16, 34, 35, 20, 33,  6,  2, 63, 51, 48,  2,\n",
      "       20, 16, 33, 27,  2, 58, 49,  2, 38, 20, 34, 35, 28, 30, 33])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab), 256, batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.GRU(1024, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(len(vocab))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 144s 2s/step - loss: 3.1252 - accuracy: 0.2376\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 146s 2s/step - loss: 2.2231 - accuracy: 0.3704\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 148s 2s/step - loss: 2.0288 - accuracy: 0.4154\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 155s 2s/step - loss: 1.8657 - accuracy: 0.4549\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 151s 2s/step - loss: 1.7349 - accuracy: 0.4892\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 151s 2s/step - loss: 1.6282 - accuracy: 0.5194\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 146s 2s/step - loss: 1.5409 - accuracy: 0.5442\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 148s 2s/step - loss: 1.4715 - accuracy: 0.5622\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 146s 2s/step - loss: 1.4159 - accuracy: 0.5772\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 146s 2s/step - loss: 1.3657 - accuracy: 0.5907\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6a2066ef0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 122s 2s/step - loss: 1.3212 - accuracy: 0.6020\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 136s 2s/step - loss: 1.2784 - accuracy: 0.6127\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 147s 2s/step - loss: 1.2397 - accuracy: 0.6237\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 148s 2s/step - loss: 1.2002 - accuracy: 0.6347\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 107s 1s/step - loss: 1.1604 - accuracy: 0.6456\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 107s 1s/step - loss: 1.1192 - accuracy: 0.6577\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 111s 1s/step - loss: 1.0780 - accuracy: 0.6697\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 109s 1s/step - loss: 1.0344 - accuracy: 0.6837\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 117s 2s/step - loss: 0.9888 - accuracy: 0.6988\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 107s 1s/step - loss: 0.9427 - accuracy: 0.7144\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd699f31550>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 96s 1s/step - loss: 0.8939 - accuracy: 0.7311\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 111s 1s/step - loss: 0.8433 - accuracy: 0.7484\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 109s 1s/step - loss: 0.7940 - accuracy: 0.7665\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 111s 1s/step - loss: 0.7441 - accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 110s 1s/step - loss: 0.6975 - accuracy: 0.8017\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 107s 1s/step - loss: 0.6498 - accuracy: 0.8204\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 112s 1s/step - loss: 0.6074 - accuracy: 0.8361\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 113s 1s/step - loss: 0.5697 - accuracy: 0.8508\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 116s 2s/step - loss: 0.5359 - accuracy: 0.8629\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 110s 1s/step - loss: 0.5043 - accuracy: 0.8745\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6bda882e8>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (64, None, 256)           17920     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (64, None, 70)            71750     \n",
      "=================================================================\n",
      "Total params: 4,027,974\n",
      "Trainable params: 4,027,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab), 256, batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(1024, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(len(vocab))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd69e7c6b00>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, None, 256)            17920     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, None, 70)             71750     \n",
      "=================================================================\n",
      "Total params: 4,027,974\n",
      "Trainable params: 4,027,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "predictive_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"SCENE II. A plain in Warwickshire.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = text[504087:504125]\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_val = [char_to_idx[c] for c in input_text]\n",
    "input_val = tf.expand_dims(input_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 38), dtype=int32, numpy=\n",
       "array([[ 1,  4, 34, 18, 20, 29, 20,  2, 24, 24,  8,  2, 16,  2, 59, 55,\n",
       "        44, 52, 57,  2, 52, 57,  2, 38, 44, 61, 66, 52, 46, 54, 62, 51,\n",
       "        52, 61, 48,  8,  4,  1]], dtype=int32)>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"SCENE II. A plain in Warwickshire.\"\n",
      "he flowerd of east our affarest,\"\n",
      "\"Commanded, with your audacions to your holy oath is but weak.\"\n",
      "\"Why, you shall.\"\n",
      "\"Peace, though I break thee go.\"\n",
      "\"To hear the troops of Earlasion lay in hand,\"\n",
      "\"A noble earl, and leave to be hanged.\"\n",
      "\"The spitious east unto his talong of ourself,\"\n",
      "\"Whom I with pain havon of you are,\"\n",
      "\"By Henry be apapto the gallant of the queen to France?\"\n",
      "\"Had sle hears Suffolk, let me proud,\"\n",
      "\"Your faithful sleep, will you leave thee to thy too:\"\n",
      "\"Then can the counterpein bu\n"
     ]
    }
   ],
   "source": [
    "predictive_model.reset_states()\n",
    "\n",
    "text_generated = []\n",
    "\n",
    "for i in range(500):\n",
    "    predictions = predictive_model(input_val)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    \n",
    "    input_val = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "print(input_text + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
